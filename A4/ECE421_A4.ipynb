{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz9MhPEUMrkC"
      },
      "source": [
        "**Assignment**: Designing and Tuning a Convolutional Neural Network (CNN)\n",
        "\n",
        "**Assignment Description**: There are four parts to this assignment\n",
        "\n",
        "1.   Building a CNN\n",
        "2.   Training and Tuning a CNN\n",
        "3.   Trying Out a New Dataset\n",
        "4.   Open-Ended Exploration\n",
        "\n",
        "You will be largely guided through the first two parts. The third and fourth part are discussion based questions. \n",
        "\n",
        "**Before the experiment, make sure that you have GPU enabled. This setting can be found under *Tools --> Settings***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHvVs2GqXxNF"
      },
      "source": [
        "#Install Objax\n",
        "!pip --quiet install  objax\n",
        "import objax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqQf8f2RBDcx"
      },
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import jax.numpy as jn\n",
        "import random \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_7vcWRFO39r"
      },
      "source": [
        "##**Part 1. Building a CNN** \n",
        "\n",
        "Before we build our CNN model, let's first import a dataset. For our experiment, we load the CIFAR10 dataset from Tensorflow's dataset repository. The CIFAR10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
        "\n",
        "After loading the dataset, we split the dataset into training, validation and test set. The dataset is originally stored as 50,000 training examples and 10,000 test examples. Instead, we will combine them together and make our own split.\n",
        "\n",
        "Do not change split ratio for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5VQDzs1XodT"
      },
      "source": [
        "#.load_data() by default returns a split between training and test set. \n",
        "# We then adjust the training set into a format that can be accepted by our CNN\n",
        "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "X_train = X_train.transpose(0, 3, 1, 2) / 255.0\n",
        "Y_train = Y_train.flatten()\n",
        "X_test = X_test.transpose(0, 3, 1, 2) / 255.0\n",
        "Y_test = Y_test.flatten()\n",
        "\n",
        "np.random.seed(1)\n",
        "# To create a validation set, we first concate the original splitted dataset into a single dataset \n",
        "# then randomly shuffle the images and labels in the same way (seed = 1)\n",
        "X_data = np.concatenate([X_train, X_test], axis = 0)\n",
        "Y_data = np.concatenate([Y_train, Y_test], axis = 0)\n",
        "\n",
        "N = np.arange(len(X_data))\n",
        "np.random.shuffle(N)\n",
        "X_data = X_data[N]\n",
        "Y_data = Y_data[N]\n",
        "\n",
        "#Next, we partition the randomly shuffled dataset into training, validation and testset according a ratio\n",
        "train_ratio = 0.80\n",
        "valid_ratio = 0.1\n",
        "n_train = int(len(X_data) * train_ratio)\n",
        "n_valid = int(len(X_data) * valid_ratio)\n",
        "\n",
        "X_train, X_valid, X_test = X_data[:n_train], X_data[n_train:n_train+n_valid], X_data[n_train+n_valid:]\n",
        "Y_train, Y_valid, Y_test = Y_data[:n_train], Y_data[n_train:n_train+n_valid], Y_data[n_train+n_valid:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szDmexFGT7Qs"
      },
      "source": [
        "\n",
        "Next we will construct a **Base Model**, which in our case is a small CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Eeh6jvfBV4p"
      },
      "source": [
        "class ConvNet(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 32, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(32, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "    x = self.conv_2(x)\n",
        "  \n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "model = ConvNet()\n",
        "#You can examine the architecture of our CNN by calling model.vars()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crz2dQZBds89"
      },
      "source": [
        "Before we train our conv net, let's try to better understand concepts of convolution filter and linear layer. In the following, you will take the first very image of the training set, create a simple convolution routine, and show that our own routine matches what Objax returns. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epa7ETf6XddH"
      },
      "source": [
        "#Let's plot the first image in the training set.\n",
        "plt.imshow(X_train[0].transpose(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQjS06vgXZ7r"
      },
      "source": [
        "Next, we will pass our image through Objax's convolution routine. Carefully examine the following code and try to understand the dimension of the filter weights and the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFBbJHBpXXUc"
      },
      "source": [
        "# We append the first image with a batch size of 1 so it can be fed into a convolution layer\n",
        "my_image = np.expand_dims(X_train[0], 0)\n",
        "\n",
        "#Consider a very simple CNN filter with stride = 1 and no padding ('VALID').\n",
        "Conv2d = objax.nn.Conv2D(nin = 3, nout = 2, k = 1, strides = 1, padding = 'VALID', use_bias = False)\n",
        "\n",
        "filter_weights = Conv2d.w.value #This is the initial weight of the filter, which we gradually update when training, we ignore bias for now\n",
        "\n",
        "print(\"Filter weights:\", filter_weights)\n",
        "print(\"Conv output:\", Conv2d(my_image))\n",
        "print(\"Conv output shape:\", np.shape(Conv2d(my_image)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsDg8yateSuH"
      },
      "source": [
        "**In the cells below, you will create your own convolution routine that takes in the image and the initial weights used by Objax's own convolution routine (Conv2d.w.value) and show that your convolution routine returns the same value than Objax's.**\n",
        "\n",
        "A simple implementation only requires 4 FOR loops. You may wish to draw inspiration from https://objax.readthedocs.io/en/latest/objax/nn.html?highlight=objax.nn.Conv2D#objax.nn.Conv2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5B6K9gyXTAx"
      },
      "source": [
        "#Solution to the above problem\n",
        "\n",
        "def my_conv_net(my_image, initial_filter_weights):\n",
        "  \"PUT YOUR CODE HERE\"\n",
        "  return my_conv_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9dbovOwiVDE"
      },
      "source": [
        "The outputs of last convolution layer is typically rearranged so it can be fed into a linear layer. Check that calling .mean((2,3)) rearranges the output of your convolution routine by examining the shape of the output. (Not graded) Think about alternative ways of rearranging the output from the convolution layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7hngIUNXIMQ"
      },
      "source": [
        "#Check that .mean((2,3)) rearranges your image\n",
        "my_conv_output.mean((2,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTWfvL0mis3D"
      },
      "source": [
        "Take your rearranged output and feed it into a linear layer of appropriate size. Here is an example:\n",
        "\n",
        "```\n",
        "Linear_Layer = objax.nn.Linear(N, 1)\n",
        "Y = Linear_Layer(X)\n",
        "```\n",
        "Next, extract the weights and bias of the linear layer using \n",
        "```\n",
        "Linear_Layer.w.value\n",
        "Linear_Layer.b.value\n",
        "```\n",
        "**Using these values, write one line of code that manually implements the linear layer. Show that it provides the same value as Objax's own linear layer.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq-TkFpgXDC7"
      },
      "source": [
        "#PUT YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96QQXl3d2kZn"
      },
      "source": [
        "You have now completed Part 1 of the assignment. Good job!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kqaH75UUaSE"
      },
      "source": [
        "##**Part 2. Training and Tuning a CNN**\n",
        "\n",
        "The following starter code trains the neural network in Part 1. However, the optimizer and batch sampling routine are left for you to implement. Complete the lines that says #PUT YOUR CODE HERE#\n",
        "\n",
        "Afterwards, train the model, and observe the training/validation loss and accuracy plots. You should observe that the validation accuracy is low and stagnates after a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBcHWoCl0URZ"
      },
      "source": [
        "#Define loss function as averaged value of of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = model(x)\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(model(x)), model.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, model.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in model.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      ####################\n",
        "      #PUT YOUR CODE HERE#\n",
        "      ####################                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGPpVTfG0Ug1"
      },
      "source": [
        "def train(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss =  0  # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0        # validation accuracy per batch\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          batch = #PUT YOUR CODE HERE#\n",
        "          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "      # run validation\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "          batch = #PUT YOUR CODE HERE#\n",
        "          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))\n",
        "  \n",
        "  #Plot training loss\n",
        "  plt.title(\"Train vs Validation Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train vs Validation Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YqWtV5VYW45"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hluN6UIbNyj7"
      },
      "source": [
        "Follow the assignment handout for questions to be answered in this part of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8LXS4oHZA9M"
      },
      "source": [
        "#PUT YOUR CODE HERE (Use as many cells as you want)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XOXmJiGZF78"
      },
      "source": [
        "You have now completed Part 2 of the assignment. Good job!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cno9fjO1o2Jq"
      },
      "source": [
        "##**Part 3. Trying Out a New Dataset**\n",
        "\n",
        "See the handout for instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eye-uBnQWgc8"
      },
      "source": [
        "##**Problem 4. Open-Ended Exploration**\n",
        "\n",
        "See the handout for instructions."
      ]
    }
  ]
}